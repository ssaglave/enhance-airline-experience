{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Part : added the following enhancements to preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TYj5rJJ1MSy"
   },
   "source": [
    "---\n",
    "\n",
    "‚úî Elongated Word Normalization (e.g., \"goooood\" ‚Üí \"good\") \n",
    "<br>‚úî Contraction Expansion (e.g., \"you're\" ‚Üí \"you are\") \n",
    "<br>‚úî POS-based Lemmatization (improves accuracy of lemmatization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suvarnaaglave/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "VykHQAW0ty_2"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"BA_Airline_Reviews_Cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "7n_z-oaLtzCV"
   },
   "outputs": [],
   "source": [
    "# Define negative words to retain\n",
    "negative_words = ['not', 'no', 'never', \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\", \"hasn't\", \"hadn't\", \"won't\", \"wouldn't\", \"don't\", \"doesn't\", \"didn't\", \"can't\", \"couldn't\", \"shouldn't\", \"mightn't\", \"mustn't\"]\n",
    "\n",
    "# Get stopwords and remove negative words\n",
    "stop_words = set(stopwords.words('english')) - set(negative_words)\n",
    "\n",
    "# Add airline-specific words to stopwords\n",
    "stop_words.update(['ba', 'british', 'airways'])\n",
    "\n",
    "# Initialize lemmatizer\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "rYyoBJgbtzE-"
   },
   "outputs": [],
   "source": [
    "# Function to convert emojis to words\n",
    "def convert_emojis(text):\n",
    "    emoji_dict = {\":)\": \"happy\", \":(\": \"sad\", \"üòÉ\": \"happy\", \"üò¢\": \"sad\", \"üò°\": \"angry\", \"üëç\": \"positive\", \"üëé\": \"negative\"}\n",
    "    for emoji, meaning in emoji_dict.items():\n",
    "        text = text.replace(emoji, meaning)\n",
    "    return text\n",
    "\n",
    "# Function to correct spelling\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# Function to normalize elongated words\n",
    "def normalize_elongated_words(text):\n",
    "    return re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    contractions_dict = {\"you're\": \"you are\", \"they're\": \"they are\", \"we're\": \"we are\", \"it's\": \"it is\", \"i'm\": \"i am\"}\n",
    "    for contraction, expanded in contractions_dict.items():\n",
    "        text = text.replace(contraction, expanded)\n",
    "    return text\n",
    "\n",
    "# Function for POS-based Lemmatization\n",
    "def pos_based_lemmatization(tokens):\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "    pos_map = {'N': 'n', 'V': 'v', 'J': 'a', 'R': 'r'}\n",
    "    return [lemmatizer.lemmatize(word, pos=pos_map.get(tag[0], 'n')) for word, tag in pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "iCRIGaDwtzHa"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    # Convert emojis to words\n",
    "    text = convert_emojis(text)\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Normalize elongated words\n",
    "    text = normalize_elongated_words(text)\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation but keep alphanumeric characters and numbers\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Spelling correction\n",
    "    text = correct_spelling(text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords but retain negative words\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # POS-based Lemmatization\n",
    "    tokens = pos_based_lemmatization(tokens)\n",
    "    # Return tokens\n",
    "    text = \" \".join(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "Bx3S6YOStzKL"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df['ReviewBody'] = df['ReviewBody'].astype(str).apply(preprocess_text)\n",
    "df['ReviewHeader'] = df['ReviewHeader'].astype(str).apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zX9T0RltzNp",
    "outputId": "205ad732-368a-4b25-eadb-721f40489f61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. File saved as BA_Airline_Reviews_Preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"BA_Airline_Reviews_Preprocessed.csv\", index=False)\n",
    "\n",
    "print(\"Preprocessing complete. File saved as BA_Airline_Reviews_Preprocessed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtDNwEdM1n0e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
